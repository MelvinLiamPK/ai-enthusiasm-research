"""
Scrape LinkedIn Posts from Verified S&P 500 Directors
=====================================================
This script uses Apify to scrape posts from VERIFIED LinkedIn profiles.

It reads the verified URLs generated by prepare_verified_urls_for_scraping.py
(score >= 70 by default) and scrapes up to 50 posts per director.

Prerequisites:
    1. Apify account with API token in .env file
    2. Verified URLs from prepare_verified_urls_for_scraping.py
    3. Install Apify client: pip install apify-client

Usage:
    python3 scrape_verified_directors.py --stats                # Check what will be scraped
    python3 scrape_verified_directors.py --run                  # Start scraping
    python3 scrape_verified_directors.py --resume               # Resume from checkpoint
    python3 scrape_verified_directors.py --prototype 10         # Test with 10 profiles
"""

import os
import sys
import json
import time
import argparse
import pandas as pd
from datetime import datetime

# Load credentials from .env file
from dotenv import load_dotenv

# Import Apify client
try:
    from apify_client import ApifyClient
except ImportError:
    print("=" * 60)
    print("ERROR: apify_client not installed!")
    print("=" * 60)
    print("\nPlease install it:")
    print("  pip install apify-client")
    print("\nOr if using venv:")
    print("  source venv/bin/activate")
    print("  pip install apify-client")
    sys.exit(1)

env_path = os.path.join(os.path.dirname(__file__), '..', '..', '.env')
load_dotenv(env_path)

APIFY_API_TOKEN = os.getenv('APIFY_API_TOKEN')

if not APIFY_API_TOKEN:
    print("=" * 60)
    print("ERROR: Missing Apify credentials in .env file!")
    print("=" * 60)
    print(f"\nLooked for .env at: {os.path.abspath(env_path)}")
    print("\nPlease ensure your .env file contains:")
    print("  APIFY_API_TOKEN=your_apify_token")
    print("\nGet your token at: https://console.apify.com/account/integrations")
    sys.exit(1)

# Initialize Apify client
apify_client = ApifyClient(APIFY_API_TOKEN)

# =========================
# Configuration
# =========================
VERIFIED_DATA_DIR = "../../data/processed/sp500_verified/"
RESULTS_DIR = "../../data/processed/sp500_linkedin_posts/"
CHECKPOINT_DIR = "../../data/processed/sp500_checkpoints/"

# Apify settings
APIFY_ACTOR = "apimaestro/linkedin-batch-profile-posts-scraper"
MAX_POSTS_PER_PROFILE = 200  # Increased to capture active posters like Arturo NuÃ±ez
BATCH_SIZE = 100  # Profiles per Apify run (avoid timeouts)

# Alternative actors (uncomment if main one doesn't work):
# APIFY_ACTOR = "anchor/linkedin-profile-scraper"
# APIFY_ACTOR = "bebity/linkedin-profile-scraper"


# =========================
# Helper Functions
# =========================

def find_verified_file(data_dir):
    """Find the most recent verified directors file."""
    if not os.path.exists(data_dir):
        return None
    
    # Look for verified files
    verified_files = sorted([
        f for f in os.listdir(data_dir)
        if f.startswith('sp500_directors_verified_') and f.endswith('.csv')
    ])
    
    if not verified_files:
        return None
    
    # Return most recent (they have score in filename)
    return os.path.join(data_dir, verified_files[-1])


def load_verified_directors(threshold=None):
    """
    Load verified directors dataset.
    
    Args:
        threshold: If provided, filter to this score threshold
    
    Returns:
        DataFrame with verified directors
    """
    verified_file = find_verified_file(VERIFIED_DATA_DIR)
    
    if not verified_file:
        print(f"âŒ No verified directors file found in {VERIFIED_DATA_DIR}")
        print("\nPlease run:")
        print("  python3 prepare_verified_urls_for_scraping.py")
        sys.exit(1)
    
    print(f"Loading verified directors from: {verified_file}")
    df = pd.read_csv(verified_file)
    
    # Filter by threshold if provided
    if threshold and 'match_score' in df.columns:
        original_len = len(df)
        df = df[df['match_score'] >= threshold]
        print(f"  Filtered to score >= {threshold}: {len(df):,} (from {original_len:,})")
    
    # Ensure we have LinkedIn URLs
    df = df[df['linkedin_url'].notna()].copy()
    
    return df


def print_stats(df):
    """Print statistics about the verified dataset."""
    print("\n" + "=" * 80)
    print("VERIFIED DIRECTORS - SCRAPING PREVIEW")
    print("=" * 80)
    
    total = len(df)
    print(f"\nTotal verified directors: {total:,}")
    
    if 'company_name' in df.columns or 'company_name_clean' in df.columns:
        company_col = 'company_name_clean' if 'company_name_clean' in df.columns else 'company_name'
        companies = df[company_col].nunique()
        print(f"Unique companies: {companies:,}")
    
    # Score distribution
    if 'match_score' in df.columns:
        print("\nScore Distribution:")
        score_ranges = [
            (100, 100, "Perfect"),
            (95, 99, "Excellent"),
            (90, 94, "Good (typical)"),
            (80, 89, "Good"),
            (70, 79, "Fair")
        ]
        for min_s, max_s, label in score_ranges:
            count = ((df['match_score'] >= min_s) & (df['match_score'] <= max_s)).sum()
            if count > 0:
                pct = 100 * count / total
                print(f"  {label:20} ({min_s}-{max_s}): {count:>6,} ({pct:>5.1f}%)")
    
    # Quality distribution
    if 'quality_flag' in df.columns:
        print("\nQuality Flags:")
        for flag, count in df['quality_flag'].value_counts().items():
            pct = 100 * count / total
            print(f"  {flag:20} {count:>6,} ({pct:>5.1f}%)")
    
    # Board keywords
    if 'board_keyword_matched' in df.columns:
        board_count = df['board_keyword_matched'].sum()
        pct = 100 * board_count / total
        print(f"\nBoard keywords in LinkedIn title: {board_count:,} ({pct:.1f}%)")
    
    # Scraping estimates
    print(f"\n" + "=" * 80)
    print("SCRAPING ESTIMATES")
    print("=" * 80)
    print(f"\nProfiles to scrape: {total:,}")
    print(f"Max posts per profile: {MAX_POSTS_PER_PROFILE}")
    print(f"Expected total posts: ~{total * MAX_POSTS_PER_PROFILE:,} (if all have 50 posts)")
    print(f"Expected actual posts: ~{int(total * MAX_POSTS_PER_PROFILE * 0.7):,} (assuming 70% have posts)")
    
    # Cost estimate
    cost_per_1000 = 5  # Rough estimate
    estimated_cost = (total / 1000) * cost_per_1000
    print(f"\nEstimated Apify cost: ${estimated_cost:.2f} (at ~${cost_per_1000}/1000 profiles)")
    print(f"Academic discount (50%): ~${estimated_cost * 0.5:.2f}")
    
    # Batch info
    n_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE
    print(f"\nWill run in {n_batches} mini-batches of {BATCH_SIZE} profiles")
    print(f"Estimated time: {n_batches * 5}-{n_batches * 15} minutes")


# =========================
# Apify Integration
# =========================

def call_apify_actor(profile_urls, max_posts=MAX_POSTS_PER_PROFILE):
    """
    Call Apify to scrape LinkedIn posts using the official ApifyClient.
    
    Args:
        profile_urls: List of LinkedIn profile URLs
        max_posts: Maximum posts to scrape per profile
    
    Returns:
        List of scraped data, or None if failed
    """
    print(f"\n    [Apify] Starting scrape for {len(profile_urls)} profiles...")
    
    # Prepare input - use "usernames" as per Apify's documentation
    run_input = {
        "usernames": profile_urls,
        "maxPosts": max_posts,
    }
    
    try:
        # Run the Actor and wait for it to finish
        print(f"    [Apify] Calling actor: {APIFY_ACTOR}")
        run = apify_client.actor(APIFY_ACTOR).call(run_input=run_input)
        
        # Get run status
        run_id = run.get('id')
        status = run.get('status')
        
        print(f"    [Apify] Run ID: {run_id}")
        print(f"    [Apify] Status: {status}")
        
        if status != 'SUCCEEDED':
            print(f"    [Apify] âŒ Run failed with status: {status}")
            return None
        
        # Fetch results from the dataset
        dataset_id = run.get('defaultDatasetId')
        print(f"    [Apify] Fetching results from dataset: {dataset_id}")
        
        # Get all items from the dataset
        results = list(apify_client.dataset(dataset_id).iterate_items())
        
        print(f"    [Apify] âœ“ Retrieved {len(results)} profile results")
        
        return results
        
    except Exception as e:
        print(f"    [Apify] âŒ Error: {e}")
        return None


def scrape_in_batches(profile_urls, checkpoint_callback=None, start_from=0):
    """
    Scrape profiles in mini-batches to avoid timeouts.
    
    Args:
        profile_urls: List of all profile URLs
        checkpoint_callback: Function to save progress
        start_from: Index to resume from
    
    Returns:
        Combined results from all batches
    """
    all_results = []
    total = len(profile_urls)
    n_batches = (total - start_from + BATCH_SIZE - 1) // BATCH_SIZE
    
    print(f"\n{'='*80}")
    print(f"SCRAPING {total:,} PROFILES IN {n_batches} MINI-BATCHES")
    print("=" * 80)
    
    for i in range(start_from, total, BATCH_SIZE):
        batch_num = (i // BATCH_SIZE) + 1
        total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE
        batch_urls = profile_urls[i:i + BATCH_SIZE]
        
        print(f"\n--- Mini-batch {batch_num}/{total_batches} ({len(batch_urls)} profiles) ---")
        
        results = call_apify_actor(batch_urls)
        
        if results:
            all_results.extend(results)
            print(f"    Running total: {len(all_results)} profiles scraped")
        else:
            print(f"    âš ï¸ Batch failed, but continuing...")
        
        # Save checkpoint
        if checkpoint_callback:
            checkpoint_callback(all_results, i + len(batch_urls))
        
        # Delay between batches
        if i + BATCH_SIZE < total:
            print("    Waiting 10s before next batch...")
            time.sleep(10)
    
    return all_results


# =========================
# Data Processing
# =========================

def parse_and_save_results(raw_data, output_dir, verified_df=None):
    """
    Parse Apify results and save in multiple formats.
    
    Creates:
        - verified_directors_posts_raw.json
        - verified_directors_posts.csv (with company metadata)
        - verified_directors_profiles.csv
    
    Args:
        raw_data: Raw results from Apify
        output_dir: Directory to save outputs
        verified_df: DataFrame with verified directors (includes company_name, ticker, director_name)
    """
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create lookup dictionary: profile_url -> list of companies (director may serve on multiple boards)
    url_to_companies = {}
    if verified_df is not None:
        for _, row in verified_df.iterrows():
            url = row['linkedin_url']
            company_info = {
                'company_name': row.get('company_name', ''),
                'ticker': row.get('ticker', ''),
                'director_name': row.get('director_name_clean', row.get('director_name', ''))
            }
            
            if url not in url_to_companies:
                url_to_companies[url] = []
            url_to_companies[url].append(company_info)
    
    # 1. Save raw JSON
    raw_path = os.path.join(output_dir, f"verified_directors_posts_raw_{timestamp}.json")
    with open(raw_path, 'w', encoding='utf-8') as f:
        json.dump(raw_data, f, indent=2, ensure_ascii=False)
    print(f"\nâœ“ Raw JSON: {raw_path}")
    
    # DEBUG: Inspect first item to understand structure
    if raw_data and len(raw_data) > 0:
        print(f"\n[DEBUG] Inspecting data structure:")
        print(f"  Total items: {len(raw_data)}")
        first_item = raw_data[0]
        print(f"  First item keys: {list(first_item.keys())}")
        if 'posts' in first_item:
            print(f"  First item has 'posts' field")
        if 'activities' in first_item:
            print(f"  First item has 'activities' field")
    
    # 2. Parse posts and profiles
    posts_list = []
    profiles_list = []
    
    for item in raw_data:
        # Try multiple field name variations for profile data
        profile_url = (item.get('profileUrl') or 
                      item.get('url') or 
                      item.get('profile_url') or 
                      item.get('linkedin_url') or '')
        
        profile_name = (item.get('fullName') or 
                       item.get('name') or 
                       item.get('full_name') or '')
        
        profile_headline = (item.get('headline') or 
                           item.get('title') or '')
        
        # Profile info
        profiles_list.append({
            'profile_url': profile_url,
            'name': profile_name,
            'headline': profile_headline,
            'followers': item.get('followersCount', item.get('followers', '')),
            'connections': item.get('connectionsCount', item.get('connections', '')),
        })
        
        # Try to find posts in multiple possible fields
        posts = (item.get('posts') or 
                item.get('activities') or 
                item.get('post') or 
                item.get('activity') or 
                [])
        
        # Handle case where the item itself IS a post
        if not posts and ('text' in item or 'content' in item or 'postUrl' in item or 'url' in item):
            # This item IS a post, not a profile with posts
            companies_list = url_to_companies.get(profile_url, [{}])
            
            # Extract engagement stats from the 'stats' object
            stats = item.get('stats', {})
            posted_at = item.get('posted_at', {})
            author = item.get('author', {})
            
            # Create one row per company this director serves on
            for company_info in companies_list:
                posts_list.append({
                    'company_name': company_info.get('company_name', ''),
                    'ticker': company_info.get('ticker', ''),
                    'director_name': company_info.get('director_name', ''),
                    'profile_url': profile_url or item.get('profile_input', ''),
                    'post_date': posted_at.get('date', item.get('postedDate', item.get('date', ''))),
                    'post_text': item.get('text', item.get('content', '')),
                    'post_url': item.get('url', item.get('postUrl', '')),
                    'likes': stats.get('like', stats.get('total_reactions', 0)),
                    'comments': stats.get('comments', 0),
                    'reposts': stats.get('reposts', 0),
                    'post_type': item.get('post_type', item.get('type', '')),
                    'total_reactions': stats.get('total_reactions', 0),
                })
        elif isinstance(posts, list):
            # Posts are in a nested list
            companies_list = url_to_companies.get(profile_url, [{}])
            
            for post in posts:
                # Extract engagement stats from the 'stats' object
                stats = post.get('stats', {})
                posted_at = post.get('posted_at', {})
                
                # Create one row per company this director serves on
                for company_info in companies_list:
                    posts_list.append({
                        'company_name': company_info.get('company_name', ''),
                        'ticker': company_info.get('ticker', ''),
                        'director_name': company_info.get('director_name', ''),
                        'profile_url': profile_url or post.get('profile_input', ''),
                        'post_date': posted_at.get('date', post.get('postedDate', post.get('date', ''))),
                        'post_text': post.get('text', post.get('content', '')),
                        'post_url': post.get('url', post.get('postUrl', '')),
                        'likes': stats.get('like', stats.get('total_reactions', 0)),
                        'comments': stats.get('comments', 0),
                        'reposts': stats.get('reposts', 0),
                        'post_type': post.get('post_type', post.get('type', '')),
                        'total_reactions': stats.get('total_reactions', 0),
                    })
    
    # 3. Save posts CSV
    if posts_list:
        posts_df = pd.DataFrame(posts_list)
        posts_path = os.path.join(output_dir, f"verified_directors_posts_{timestamp}.csv")
        posts_df.to_csv(posts_path, index=False, encoding='utf-8')
        print(f"âœ“ Posts CSV: {posts_path} ({len(posts_df):,} posts)")
    else:
        posts_path = None
        print("âš ï¸ No posts found - check raw JSON to see data structure")
    
    # 4. Save profiles CSV
    if profiles_list:
        profiles_df = pd.DataFrame(profiles_list)
        profiles_path = os.path.join(output_dir, f"verified_directors_profiles_{timestamp}.csv")
        profiles_df.to_csv(profiles_path, index=False, encoding='utf-8')
        print(f"âœ“ Profiles CSV: {profiles_path} ({len(profiles_df):,} profiles)")
    else:
        profiles_path = None
    
    return {
        'raw_path': raw_path,
        'posts_path': posts_path,
        'profiles_path': profiles_path,
        'posts_count': len(posts_list),
        'profiles_count': len(profiles_list),
        'timestamp': timestamp
    }


# =========================
# Checkpoint Management
# =========================

def save_checkpoint(results, profiles_processed, output_dir):
    """Save scraping checkpoint."""
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    
    # Save temporary results
    temp_file = os.path.join(output_dir, "temp_results.json")
    with open(temp_file, 'w') as f:
        json.dump(results, f)
    
    # Save checkpoint metadata
    checkpoint_file = os.path.join(CHECKPOINT_DIR, "verified_scraping_checkpoint.json")
    with open(checkpoint_file, 'w') as f:
        json.dump({
            'profiles_processed': profiles_processed,
            'results_file': temp_file,
            'timestamp': datetime.now().isoformat()
        }, f)
    
    print(f"    ðŸ’¾ Checkpoint: {profiles_processed} profiles done")


def load_checkpoint():
    """Load scraping checkpoint."""
    checkpoint_file = os.path.join(CHECKPOINT_DIR, "verified_scraping_checkpoint.json")
    
    if not os.path.exists(checkpoint_file):
        return None
    
    with open(checkpoint_file, 'r') as f:
        checkpoint = json.load(f)
    
    # Load previous results
    temp_file = checkpoint.get('results_file')
    if temp_file and os.path.exists(temp_file):
        with open(temp_file, 'r') as f:
            results = json.load(f)
        checkpoint['previous_results'] = results
    else:
        checkpoint['previous_results'] = []
    
    return checkpoint


def clear_checkpoint():
    """Clear checkpoint after successful completion."""
    checkpoint_file = os.path.join(CHECKPOINT_DIR, "verified_scraping_checkpoint.json")
    if os.path.exists(checkpoint_file):
        os.remove(checkpoint_file)
    
    # Clean up temp files
    temp_file = os.path.join(RESULTS_DIR, "temp_results.json")
    if os.path.exists(temp_file):
        os.remove(temp_file)


# =========================
# Main Scraping Logic
# =========================

def run_scraping(threshold=None, resume=True, prototype_limit=None):
    """
    Main scraping function.
    
    Args:
        threshold: Filter to this score threshold
        resume: Resume from checkpoint if available
        prototype_limit: If set, only scrape this many profiles (for testing)
    """
    # Load verified directors
    df = load_verified_directors(threshold)
    
    print(f"\nâœ“ Loaded {len(df):,} verified directors")
    
    # Apply prototype limit
    if prototype_limit:
        df = df.head(prototype_limit)
        print(f"  PROTOTYPE MODE: Limited to {len(df)} profiles")
    
    # Get profile URLs and remove duplicates (some directors serve on multiple boards)
    profile_urls_raw = df['linkedin_url'].tolist()
    profile_urls = list(dict.fromkeys(profile_urls_raw))  # Preserves order, removes duplicates
    
    if len(profile_urls) < len(profile_urls_raw):
        duplicates_removed = len(profile_urls_raw) - len(profile_urls)
        print(f"\nâš ï¸  Removed {duplicates_removed} duplicate LinkedIn URLs")
        print(f"   (Some directors serve on multiple S&P 500 boards)")
        print(f"   Unique profiles to scrape: {len(profile_urls):,}")
    
    # Check for checkpoint
    start_from = 0
    previous_results = []
    
    if resume:
        checkpoint = load_checkpoint()
        if checkpoint:
            start_from = checkpoint.get('profiles_processed', 0)
            previous_results = checkpoint.get('previous_results', [])
            print(f"\nâœ“ Resuming from checkpoint: {start_from} profiles already done")
    
    if start_from >= len(profile_urls):
        print("\nâœ“ All profiles already scraped!")
        return
    
    # Create output directory
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    # Checkpoint callback
    def checkpoint_callback(results, profiles_done):
        save_checkpoint(results, profiles_done, RESULTS_DIR)
    
    # Scrape
    remaining_urls = profile_urls[start_from:]
    new_results = scrape_in_batches(
        remaining_urls,
        checkpoint_callback=checkpoint_callback,
        start_from=0
    )
    
    # Combine with previous results
    all_results = previous_results + new_results
    
    # Save final results
    print(f"\n{'='*80}")
    print("SAVING FINAL RESULTS")
    print("=" * 80)
    
    save_info = parse_and_save_results(all_results, RESULTS_DIR, verified_df=df)
    
    # Clear checkpoint
    clear_checkpoint()
    
    # Summary
    print(f"\n{'='*80}")
    print("âœ… SCRAPING COMPLETE")
    print("=" * 80)
    print(f"\nProfiles scraped: {save_info['profiles_count']:,}")
    print(f"Posts collected: {save_info['posts_count']:,}")
    print(f"\nOutput files:")
    print(f"  Posts: {save_info['posts_path']}")
    print(f"  Profiles: {save_info['profiles_path']}")
    print(f"  Raw JSON: {save_info['raw_path']}")
    
    # Next steps
    print(f"\n{'='*80}")
    print("NEXT STEPS")
    print("=" * 80)
    print("\n1. Analyze posts for AI keywords:")
    print("   python3 analyze_ai_enthusiasm.py")
    print("\n2. Merge with director/company metadata")
    print("\n3. Calculate AI enthusiasm scores per director")


# =========================
# Main
# =========================

def main():
    parser = argparse.ArgumentParser(
        description='Scrape LinkedIn posts from verified S&P 500 directors',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python3 scrape_verified_directors.py --stats              # Preview what will be scraped
  python3 scrape_verified_directors.py --run                # Start scraping
  python3 scrape_verified_directors.py --resume             # Resume from checkpoint
  python3 scrape_verified_directors.py --prototype 10       # Test with 10 profiles
  python3 scrape_verified_directors.py --threshold 80       # Only score >= 80

Prerequisites:
  1. Run prepare_verified_urls_for_scraping.py first
  2. Ensure APIFY_API_TOKEN is set in .env file

Cost: ~$5 per 1,000 profiles (academic discount: 50% off)
        """
    )
    
    parser.add_argument('--stats', action='store_true',
                        help='Show statistics about verified directors')
    parser.add_argument('--run', action='store_true',
                        help='Start scraping')
    parser.add_argument('--resume', action='store_true',
                        help='Resume from checkpoint')
    parser.add_argument('--prototype', type=int, metavar='N',
                        help='Test mode: scrape only N profiles')
    parser.add_argument('--threshold', type=int,
                        help='Filter to score >= threshold')
    parser.add_argument('--no-resume', action='store_true',
                        help='Start from beginning (ignore checkpoint)')
    
    args = parser.parse_args()
    
    # Load and show stats
    df = load_verified_directors(args.threshold)
    print_stats(df)
    
    if args.stats:
        print("\nâœ“ Statistics displayed. Use --run to start scraping.")
        return
    
    if args.prototype:
        print(f"\n{'='*80}")
        print(f"PROTOTYPE MODE - {args.prototype} PROFILES")
        print("=" * 80)
        confirm = input(f"\nScrape {args.prototype} profiles? (y/N): ").strip().lower()
        if confirm == 'y':
            run_scraping(
                threshold=args.threshold,
                resume=False,
                prototype_limit=args.prototype
            )
        return
    
    if args.run or args.resume:
        resume = not args.no_resume
        
        print(f"\n{'='*80}")
        print(f"READY TO SCRAPE {len(df):,} VERIFIED DIRECTORS")
        print("=" * 80)
        
        if not args.resume:
            confirm = input("\nStart scraping? (y/N): ").strip().lower()
            if confirm != 'y':
                print("Cancelled.")
                return
        
        run_scraping(
            threshold=args.threshold,
            resume=resume
        )
        return
    
    # No action specified
    parser.print_help()
    print("\n" + "=" * 80)
    print("Quick start:")
    print("  1. python3 scrape_verified_directors.py --stats    # Preview")
    print("  2. python3 scrape_verified_directors.py --run      # Start scraping")


if __name__ == "__main__":
    main()